#!/usr/bin/env python3
"""
Model Trainer Module for TradingJii ML

This module trains machine learning models using datasets generated by dataset_generator.py:
- Loads training data from CSV files
- Trains classification models for trading signals
- Validates model performance
- Saves trained models with metadata
- Supports multiple model types and hyperparameter tuning
"""

import os
import logging
import pickle
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from colorama import Fore, Style

# ML Libraries
try:
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.svm import SVC
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    import joblib
    
    sklearn_available = True
except ImportError:
    logging.warning("Scikit-learn not available. Install with: pip install scikit-learn")
    sklearn_available = False

# Import ML config
from .config import (
    MODEL_CONFIG,
    LOGGING_CONFIG,
    VALIDATION_CONFIG,
    FALLBACK_CONFIG,
    generate_schema_hash,
    get_model_config,
    MLConfigError
)

# Import dataset generator utilities
from modules.data.dataset_generator import export_supervised_training_data

class ModelTrainer:
    """
    Main class for training ML models on volatility pattern data.
    """
    
    def __init__(self, data_dir: str = "datasets", models_dir: str = "models"):
        """
        Initialize the model trainer.
        
        Args:
            data_dir: Directory containing training datasets
            models_dir: Directory to save trained models
        """
        self.data_dir = data_dir
        self.models_dir = models_dir
        self.scalers = {}  # Store scalers for each model
        
        # Create models directory if it doesn't exist
        os.makedirs(models_dir, exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        
        # Available model types
        self.model_types = {
            "random_forest": RandomForestClassifier,
            "gradient_boosting": GradientBoostingClassifier,
            "svm": SVC,
            "logistic_regression": LogisticRegression
        } if sklearn_available else {}

    def setup_logging(self):
        """Setup logging for model training."""
        if LOGGING_CONFIG["enable_file_logging"]:
            log_file = LOGGING_CONFIG["predictor_log"]
            logging.basicConfig(
                filename=log_file,
                level=getattr(logging, LOGGING_CONFIG["log_level"]),
                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )

    def load_training_data(self, symbol: str, timeframe: str) -> Dict[str, pd.DataFrame]:
        """
        Load training data from CSV files generated by dataset_generator.
        
        Args:
            symbol: Cryptocurrency symbol
            timeframe: Timeframe
            
        Returns:
            Dictionary mapping pattern categories to DataFrames
        """
        try:
            # Construct data path like in dataset_generator
            symbol_safe = symbol.replace('/', '_').replace(':', '').replace('\\', '').replace('*', '')
            symbol_safe = symbol_safe.replace('?', '').replace('"', '').replace('<', '').replace('>', '')
            symbol_safe = symbol_safe.replace('|', '')
            
            data_path = os.path.join(self.data_dir, symbol_safe, timeframe)
            
            if not os.path.exists(data_path):
                logging.error(f"Training data directory not found: {data_path}")
                return {}
            
            # Load all CSV files in the directory
            datasets = {}
            csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]
            
            if not csv_files:
                logging.error(f"No CSV files found in {data_path}")
                return {}
            
            for csv_file in csv_files:
                # Extract pattern from filename (cat_PATTERN.csv)
                if csv_file.startswith('cat_') and csv_file.endswith('.csv'):
                    pattern = csv_file[4:-4]  # Remove 'cat_' prefix and '.csv' suffix
                    
                    file_path = os.path.join(data_path, csv_file)
                    df = pd.read_csv(file_path)
                    
                    if not df.empty:
                        datasets[pattern] = df
                        logging.info(f"Loaded {len(df)} samples for pattern '{pattern}' from {csv_file}")
                    else:
                        logging.warning(f"Empty dataset file: {csv_file}")
                else:
                    logging.warning(f"Skipping non-pattern file: {csv_file}")
            
            logging.info(f"Loaded {len(datasets)} pattern datasets for {symbol} ({timeframe})")
            return datasets
            
        except Exception as e:
            logging.error(f"Error loading training data for {symbol} ({timeframe}): {e}")
            return {}

    def prepare_training_data(self, datasets: Dict[str, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Prepare training data from pattern datasets.
        
        Args:
            datasets: Dictionary of pattern datasets
            
        Returns:
            Tuple of (features, labels, feature_names)
        """
        try:
            all_features = []
            all_labels = []
            feature_names = []
            
            for pattern, df in datasets.items():
                # Extract feature columns (x_1, x_2, ..., x_n)
                feature_cols = [col for col in df.columns if col.startswith('x_')]
                feature_cols.sort(key=lambda x: int(x.split('_')[1]))  # Sort by number
                
                if not feature_cols:
                    logging.warning(f"No feature columns found for pattern {pattern}")
                    continue
                
                # Set feature names from first dataset
                if not feature_names:
                    feature_names = feature_cols
                
                # Extract features and convert pattern to classification labels
                features = df[feature_cols].values
                
                # Convert patterns to numeric labels for classification
                # We'll create a 3-class problem: BUY(1), SELL(-1), HOLD(0)
                labels = self.pattern_to_signal(pattern, len(features))
                
                all_features.append(features)
                all_labels.extend(labels)
                
                logging.debug(f"Pattern {pattern}: {len(features)} samples -> signal {labels[0] if labels else 'none'}")
            
            if not all_features:
                raise ValueError("No valid feature data found")
            
            # Combine all features and labels
            X = np.vstack(all_features)
            y = np.array(all_labels)
            
            logging.info(f"Prepared training data: {X.shape[0]} samples, {X.shape[1]} features")
            logging.info(f"Label distribution: {dict(zip(*np.unique(y, return_counts=True)))}")
            
            return X, y, feature_names
            
        except Exception as e:
            logging.error(f"Error preparing training data: {e}")
            raise

    def pattern_to_signal(self, pattern: str, num_samples: int) -> List[int]:
        """
        Convert volatility pattern to trading signal labels.
        
        PATTERN DOCUMENTATION:
        =====================
        
        Patterns are 7-character binary strings representing volatility behavior over 7 consecutive periods.
        Each character represents a time period:
        - '1' = High/Rising volatility period (above threshold)
        - '0' = Low/Stable volatility period (below threshold)
        
        SOURCE: These patterns are generated by modules/data/volatility_processor.py using statistical 
        analysis of rolling volatility windows. The binary categorization is based on:
        - Rolling standard deviation of price returns
        - Comparison against dynamic volatility thresholds  
        - 7-period lookback window for pattern formation
        
        CRITERIA FOR PATTERN CLASSIFICATION:
        - Volatility threshold: 2-standard deviation from rolling mean
        - Minimum pattern length: 7 periods for statistical significance
        - Pattern stability: Consecutive periods must maintain classification
        
        TRADING SIGNAL HEURISTICS:
        - Rising volatility often precedes price breakouts (bullish)
        - Falling volatility may indicate consolidation or breakdown (bearish) 
        - Mixed patterns suggest ranging/uncertain markets (neutral)
        
        Args:
            pattern: 7-character binary pattern string from volatility categorization
            num_samples: Number of samples to create labels for
            
        Returns:
            List of signal labels (1=BUY, 0=HOLD, -1=SELL)
        """
        
        # Documented pattern mapping based on volatility behavior analysis
        pattern_mapping = {
            # === BULLISH PATTERNS (Rising Volatility Trends) ===
            # Strong uptrend in volatility -> Potential breakout signal
            "1111111": 1,  # Consistently rising volatility (7/7 periods high)
            "0111111": 1,  # Strong acceleration (6/7 periods high, trending up)
            "0011111": 1,  # Rising trend (5/7 periods high, clear upward momentum)
            "0001111": 1,  # Accelerating upward (4/7 periods high, building momentum)
            
            # === BEARISH PATTERNS (Falling Volatility Trends) ===  
            # Strong downtrend in volatility -> Potential breakdown signal
            "0000000": -1,  # Consistently falling volatility (0/7 periods high)
            "1000000": -1,  # Strong deceleration (1/7 periods high, trending down)
            "1100000": -1,  # Falling trend (2/7 periods high, clear downward momentum)
            "1110000": -1,  # Accelerating downward (3/7 periods high, losing momentum)
            
            # === NEUTRAL PATTERNS (Mixed/Oscillating Volatility) ===
            # Uncertain/ranging volatility -> Hold signal
            "0101010": 0,   # Regular oscillation (alternating high/low periods)
            "1010101": 0,   # Regular oscillation (alternating low/high periods)
            "0110110": 0,   # Mixed pattern with no clear trend
            "1001001": 0,   # Irregular oscillation pattern
            "0100100": 0,   # Sporadic volatility spikes
        }
        
        # Intelligent fallback for unmapped patterns
        if pattern in pattern_mapping:
            signal = pattern_mapping[pattern]
        else:
            # Statistical heuristic based on pattern composition
            ones_count = pattern.count('1')
            zeros_count = pattern.count('0')
            total_periods = len(pattern)
            
            # Calculate volatility trend strength
            volatility_ratio = ones_count / total_periods
            
            # Trend detection: check if pattern shows momentum
            # Look at recent periods (last 4) vs early periods (first 3)
            recent_volatility = pattern[-4:].count('1') / 4
            early_volatility = pattern[:3].count('1') / 3
            momentum = recent_volatility - early_volatility
            
            # Signal classification with momentum consideration
            if volatility_ratio >= 0.7 and momentum >= 0:  # High volatility with positive momentum
                signal = 1  # BUY - expect breakout
            elif volatility_ratio <= 0.3 and momentum <= 0:  # Low volatility with negative momentum  
                signal = -1  # SELL - expect breakdown
            elif abs(momentum) >= 0.4:  # Strong momentum in either direction
                signal = 1 if momentum > 0 else -1
            else:
                signal = 0  # HOLD - uncertain/mixed pattern
        
        return [signal] * num_samples

    def train_model(
        self,
        X: np.ndarray,
        y: np.ndarray,
        model_type: str = "random_forest",
        hyperparameters: Dict = None,
        test_size: float = 0.2,
        random_state: int = 42
    ) -> Tuple[Any, Optional[Any], Dict]:
        """
        Train a classification model.
        
        Args:
            X: Feature matrix
            y: Target labels
            model_type: Type of model to train
            hyperparameters: Model hyperparameters
            test_size: Proportion of test data
            random_state: Random state for reproducibility
            
        Returns:
            Tuple of (trained_model, metrics)
        """
        try:
            if not sklearn_available:
                raise MLConfigError("Scikit-learn not available for model training")
            
            if model_type not in self.model_types:
                raise ValueError(f"Unknown model type: {model_type}")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=y
            )
            
            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Default hyperparameters
            default_params = {
                "random_forest": {"n_estimators": 100, "max_depth": 10, "random_state": random_state},
                "gradient_boosting": {"n_estimators": 100, "max_depth": 6, "random_state": random_state},
                "svm": {"kernel": "rbf", "C": 1.0, "probability": True, "random_state": random_state},
                "logistic_regression": {"C": 1.0, "random_state": random_state, "max_iter": 1000}
            }
            
            params = hyperparameters or default_params.get(model_type, {})
            
            # Initialize and train model
            model_class = self.model_types[model_type]
            model = model_class(**params)
            
            logging.info(f"Training {model_type} model with parameters: {params}")
            
            # Use scaled features for SVM and Logistic Regression, raw for tree-based models
            if model_type in ["svm", "logistic_regression"]:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
                use_scaler = True
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
                use_scaler = False
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            
            # Cross-validation score
            cv_data = X_train_scaled if use_scaler else X_train
            cv_scores = cross_val_score(model, cv_data, y_train, cv=5)
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            # Classification report
            class_report = classification_report(y_test, y_pred, output_dict=True)
            
            metrics = {
                "accuracy": accuracy,
                "cv_mean": cv_mean,
                "cv_std": cv_std,
                "classification_report": class_report,
                "test_samples": len(y_test),
                "train_samples": len(y_train),
                "model_type": model_type,
                "hyperparameters": params,
                "use_scaler": use_scaler,
                "feature_count": X.shape[1]
            }
            
            logging.info(f"Model training completed:")
            logging.info(f"  Accuracy: {accuracy:.4f}")
            logging.info(f"  CV Score: {cv_mean:.4f} ± {cv_std:.4f}")
            
            return model, scaler if use_scaler else None, metrics
            
        except Exception as e:
            logging.error(f"Error training model: {e}")
            raise

    def save_model(
        self,
        model: Any,
        scaler: Optional[Any],
        metrics: Dict,
        feature_names: List[str],
        model_name: str,
        symbol: str,
        timeframe: str
    ) -> str:
        """
        Save trained model with metadata.
        
        Args:
            model: Trained model
            scaler: Feature scaler (if used)
            metrics: Training metrics
            feature_names: List of feature column names
            model_name: Name for the saved model
            symbol: Symbol used for training
            timeframe: Timeframe used for training
            
        Returns:
            Path to saved model file
        """
        try:
            # Generate model filename with version
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"{model_name}_{timestamp}.pkl"
            model_path = os.path.join(self.models_dir, model_filename)
            
            # Prepare model package
            model_package = {
                "model": model,
                "scaler": scaler,
                "metrics": metrics,
                "feature_names": feature_names,
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "model_name": model_name,
                    "version": timestamp,
                    "feature_schema_hash": generate_schema_hash(
                        pd.Series([np.float64] * len(feature_names), index=feature_names),
                        feature_names
                    )
                }
            }
            
            # Save using joblib for better scikit-learn compatibility
            joblib.dump(model_package, model_path)
            
            # Also save metrics as JSON for easy reading
            metrics_path = model_path.replace('.pkl', '_metrics.json')
            with open(metrics_path, 'w') as f:
                # Convert numpy types to native Python types for JSON serialization
                serializable_metrics = self._make_json_serializable(metrics)
                json.dump(serializable_metrics, f, indent=2)
            
            logging.info(f"Model saved: {model_path}")
            logging.info(f"Metrics saved: {metrics_path}")
            
            return model_path
            
        except Exception as e:
            logging.error(f"Error saving model: {e}")
            raise

    def _make_json_serializable(self, obj):
        """Convert numpy types to Python native types for JSON serialization."""
        if isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    def train_volatility_classifier(
        self,
        symbol: str,
        timeframe: str,
        model_type: str = "random_forest"
    ) -> Optional[str]:
        """
        Train a volatility pattern classifier for a specific symbol and timeframe.
        
        Args:
            symbol: Cryptocurrency symbol
            timeframe: Timeframe
            model_type: Type of model to train
            
        Returns:
            Path to saved model or None if training failed
        """
        try:
            logging.info(f"Training volatility classifier for {symbol} ({timeframe})")
            
            # Load training data
            datasets = self.load_training_data(symbol, timeframe)
            if not datasets:
                logging.error(f"No training data available for {symbol} ({timeframe})")
                return None
            
            # Prepare training data
            X, y, feature_names = self.prepare_training_data(datasets)
            
            # Train model
            model, scaler, metrics = self.train_model(X, y, model_type)
            
            # Save model
            model_name = f"volatility_classifier_{symbol.replace('/', '_').replace(':', '')}"
            model_path = self.save_model(model, scaler, metrics, feature_names, model_name, symbol, timeframe)
            
            logging.info(f"Successfully trained volatility classifier: {model_path}")
            return model_path
            
        except Exception as e:
            logging.error(f"Error training volatility classifier: {e}")
            return None

    def train_all_models(
        self,
        symbols: List[str],
        timeframes: List[str], 
        model_types: List[str] = None
    ) -> Dict[str, List[str]]:
        """
        Train models for multiple symbols and timeframes.
        
        Args:
            symbols: List of symbols
            timeframes: List of timeframes
            model_types: List of model types to train
            
        Returns:
            Dictionary mapping (symbol, timeframe) to list of trained model paths
        """
        if model_types is None:
            model_types = ["random_forest"]
        
        results = {}
        
        for symbol in symbols:
            for timeframe in timeframes:
                key = f"{symbol}_{timeframe}"
                results[key] = []
                
                for model_type in model_types:
                    try:
                        model_path = self.train_volatility_classifier(symbol, timeframe, model_type)
                        if model_path:
                            results[key].append(model_path)
                    except Exception as e:
                        logging.error(f"Failed to train {model_type} for {symbol} ({timeframe}): {e}")
        
        return results


def train_model_from_cli(
    symbol: str,
    timeframe: str,
    model_type: str = "random_forest",
    data_dir: str = "datasets",
    models_dir: str = "models"
) -> Optional[str]:
    """
    CLI wrapper function to train a single model.
    
    Args:
        symbol: Cryptocurrency symbol
        timeframe: Timeframe
        model_type: Type of model to train
        data_dir: Directory containing datasets
        models_dir: Directory to save models
        
    Returns:
        Path to trained model or None if failed
    """
    try:
        trainer = ModelTrainer(data_dir, models_dir)
        return trainer.train_volatility_classifier(symbol, timeframe, model_type)
    except Exception as e:
        logging.error(f"CLI training failed: {e}")
        return None


def generate_datasets_and_train(
    symbols: List[str],
    timeframes: List[str],
    data_dir: str = "datasets",
    models_dir: str = "models",
    force_regeneration: bool = False
) -> Dict[str, List[str]]:
    """
    Generate datasets and train models in one workflow.
    
    Args:
        symbols: List of symbols
        timeframes: List of timeframes
        data_dir: Directory for datasets
        models_dir: Directory for models
        force_regeneration: Force dataset regeneration
        
    Returns:
        Dictionary of trained model paths
    """
    try:
        # First, generate datasets using existing dataset_generator
        from modules.data.dataset_generator import export_all_supervised_data
        
        logging.info("Generating training datasets...")
        export_results = export_all_supervised_data(
            symbols=symbols,
            timeframes=timeframes,
            output_dir=data_dir,
            force_regeneration=force_regeneration
        )
        
        if not export_results:
            logging.error("No datasets were generated")
            return {}
        
        # Then train models
        logging.info("Training models...")
        trainer = ModelTrainer(data_dir, models_dir)
        training_results = trainer.train_all_models(symbols, timeframes)
        
        return training_results
        
    except Exception as e:
        logging.error(f"Error in dataset generation and training workflow: {e}")
        return {}


if __name__ == "__main__":
    # Example usage
    import sys
    from modules.utils.logging_setup import setup_logging
    
    # Setup logging
    setup_logging(level=logging.INFO)
    
    # Default parameters
    symbols = ["BTC/USDT:USDT", "ETH/USDT:USDT", "SOL/USDT:USDT"]
    timeframes = ["1h", "4h"]
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--generate-and-train":
            # Generate datasets and train models
            results = generate_datasets_and_train(symbols, timeframes, force_regeneration=True)
            print(f"Training completed. Results: {results}")
        elif sys.argv[1] == "--train-single" and len(sys.argv) >= 4:
            # Train single model
            symbol = sys.argv[2]
            timeframe = sys.argv[3]
            model_type = sys.argv[4] if len(sys.argv) > 4 else "random_forest"
            
            model_path = train_model_from_cli(symbol, timeframe, model_type)
            if model_path:
                print(f"Model trained successfully: {model_path}")
            else:
                print("Model training failed")
        else:
            print("Usage:")
            print("  python model_trainer.py --generate-and-train")
            print("  python model_trainer.py --train-single SYMBOL TIMEFRAME [MODEL_TYPE]")
    else:
        # Default: train models for configured symbols
        trainer = ModelTrainer()
        results = trainer.train_all_models(symbols, timeframes)
        print(f"Training completed. Results: {results}")
